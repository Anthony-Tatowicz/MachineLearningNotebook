{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from six.moves import xrange\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import math\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "DATA_FILE_PATH = '../data/statefarmchallenge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to recoreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert(images, labels, name):\n",
    "    '''\n",
    "    Convert \n",
    "    '''\n",
    "    num_examples = labels.shape[0]\n",
    "    if images.shape[0] != num_examples:\n",
    "        raise ValueError(\"images size {} did not match labels size {}\"\n",
    "                         .format(images.shape[0], num_examples))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "def main():"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-1-62efe6f51668>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-62efe6f51668>\"\u001b[1;36m, line \u001b[1;32m22\u001b[0m\n\u001b[1;33m    def input_pipeline():\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def read_file(queue):\n",
    "    # Reader to read csv rows\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(queue)\n",
    "    \n",
    "    record_defaults = [[''], [''], ['']]\n",
    "    subject, label, img_file = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "def process_images(image):\n",
    "    img_file = tf.read_file(path)\n",
    "    img_data = tf.image.decode_jpeg(img_file, channels=3)\n",
    "    resized = tf.image.resize_images(img_data, 256, 256)\n",
    "    # scaled_croped = tf.image.crop_to_bounding_box(scaled, 0, 43, 256, 256)\n",
    "\n",
    "    # Convert from [0, 255] -> [-0.5, 0.5] floats.\n",
    "    image = tf.cast(resized, tf.float32) * (1. / 255.) - 0.5\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "    \n",
    "def input_pipeline():\n",
    "    # Create queue for csv file\n",
    "    file_queue = tf.train.string_input_producer([DATA_FILE_PATH], num_epochs=num_epochs)\n",
    "\n",
    "    image, label = decode_and_process_img(file_queue)\n",
    "\n",
    "    images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size, \n",
    "                                            num_threads=2,\n",
    "                                            capacity=1000 + 3 * batch_size,\n",
    "                                            min_after_dequeue=1000)\n",
    "\n",
    "    return image_batch, label_batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decode_and_process_img(queue):\n",
    "    # Reader to read csv rows\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(queue)\n",
    "    \n",
    "    record_defaults = [[''], [''], ['']]\n",
    "    subject, label, img_file = tf.decode_csv(value, record_defaults=record_defaults)\n",
    "    \n",
    "    path = tf.cast('/data/SFC/imgs/train/' + label + '/' + img_file, tf.string)\n",
    "    img_file = tf.read_file(path)\n",
    "    img_data = tf.image.decode_jpeg(img_file, channels=3)\n",
    "    resized = tf.image.resize_images(img_data, 256, 256)\n",
    "#    scaled_croped = tf.image.crop_to_bounding_box(scaled, 0, 43, 256, 256)\n",
    "\n",
    "    # Convert from [0, 255] -> [-0.5, 0.5] floats.\n",
    "    image = tf.cast(resized, tf.float32) * (1. / 255.) - 0.5\n",
    "\n",
    "    return image, label\n",
    "\n",
    "    \n",
    "def inputs(batch_size, num_epochs):\n",
    "    with tf.name_scope('input'):\n",
    "        # Create queue for csv file\n",
    "        file_queue = tf.train.string_input_producer([DATA_FILE_PATH], num_epochs=num_epochs)\n",
    "\n",
    "        image, label = decode_and_process_img(file_queue)\n",
    "        \n",
    "        images, labels = tf.train.shuffle_batch([image, label], batch_size=batch_size, \n",
    "                                                num_threads=2,\n",
    "                                                capacity=1000 + 3 * batch_size,\n",
    "                                                min_after_dequeue=1000)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tf records to run our model as a threaded queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Auxilary functions\n",
    "'''\n",
    "\n",
    "\n",
    "def variabel_on_cpu():\n",
    "    with tf.device('\\cpu0:'):\n",
    "        \n",
    "    \n",
    "    return tf.Variable()\n",
    "\n",
    "\n",
    "def variable_with_weight_decay():\n",
    "    return var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    " - add norms to layers\n",
    " - add images to summary\n",
    " - change learning rate to exp step\n",
    " - normalize images\n",
    " - experiment with color/grayscale\n",
    " - examine learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "batch_size = 128\n",
    "patch_size = 10\n",
    "depth1 = 32\n",
    "depth2 = 64\n",
    "depth3 = 64\n",
    "num_hidden = 2048\n",
    "learning_rate = .01\n",
    "num_channels = 1\n",
    "image_size = 256 * 256\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "def inference(images, keep_prob):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    images: Images placeholder, from inputs().\n",
    "    Returns:\n",
    "    softmax_linear: Output tensor with the computed logits.\n",
    "    \"\"\"\n",
    "\n",
    "    # Conv1 /w pooling\n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(\n",
    "              [patch_size, patch_size, num_channels, depth1], stddev=0.01))\n",
    "        biases = tf.Variable(tf.zeros([depth1]))\n",
    "        conv = tf.nn.conv2d(images, weights, [1, 1, 1 ,1], padding='SAME', name='conv1')\n",
    "        conv1 = tf.nn.relu(conv, name=scope)\n",
    "        \n",
    "    pool1 = tf.nn.avg_pool(conv1, [1, 6, 6, 1], [1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "    norm1 = tf.nn.local_response_normalization(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "\n",
    "    # Conv2 /w pooling \n",
    "    with tf.name_scope('conv2') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(\n",
    "              [patch_size, patch_size, depth1, depth2], stddev=0.01))\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[depth2]))\n",
    "        conv = tf.nn.conv2d(norm1, weights, [1, 1, 1, 1], padding='SAME', name='conv2')\n",
    "        conv2 = tf.nn.relu(conv, name=scope)\n",
    "    \n",
    "    pool2 = tf.nn.avg_pool(conv2, [1, 6, 6, 1], [1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "    norm2 = tf.nn.local_response_normalization(pool2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    \n",
    "    # Conv3 /w pooling\n",
    "    with tf.name_scope('conv3') as scope:\n",
    "        weights = tf.Variable(tf.truncated_normal(\n",
    "              [patch_size, patch_size, depth2, depth3], stddev=0.01))\n",
    "        biases = tf.Variable(tf.constant(1.0, shape=[depth3]))\n",
    "        conv = tf.nn.conv2d(norm2, weights, [1, 1, 1, 1], padding='SAME', name='conv3')\n",
    "        conv3 = tf.nn.relu(conv, name=scope)\n",
    "    \n",
    "    pool3 = tf.nn.avg_pool(conv3, [1, 6, 6, 1], [1, 2, 2, 1], padding='SAME', name='pool3')\n",
    "    norm3 = tf.nn.local_response_normalization(pool3, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm3') \n",
    "    shape = norm3.get_shape().as_list()\n",
    "    reshape = tf.reshape(norm3, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        \n",
    "    #local4\n",
    "    with tf.name_scope('local3') as scope:\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([image_size, num_hidden],\n",
    "                                stddev=1.0 / math.sqrt(float(num_hidden))))\n",
    "\n",
    "        biases = tf.Variable(tf.zeros([num_hidden]))\n",
    "        z = tf.matmul(reshape, weights) + biases\n",
    "        local4 = tf.nn.relu(z, name=scope)\n",
    "        drop = tf.nn.dropout(local4, keep_prob)\n",
    "\n",
    "    # local5\n",
    "    with tf.name_scope('local4') as scope:\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden, num_hidden],\n",
    "                                stddev=1.0 / math.sqrt(float(num_hidden))), name='weights_5')\n",
    "\n",
    "        biases = tf.Variable(tf.zeros([num_hidden]), name='biases_5')\n",
    "        z = tf.matmul(drop, weights) + biases\n",
    "        local5 = tf.nn.relu(z, name=scope)\n",
    "        drop = tf.nn.dropout(local5, keep_prob)\n",
    "        \n",
    "        \n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden, NUM_CLASSES],\n",
    "                                stddev=1.0 / math.sqrt(float(num_hidden))), name='weights_s')\n",
    "\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_s')\n",
    "        logits = tf.matmul(drop, weights) + biases\n",
    "        \n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "    \"\"\"\"\n",
    "    Calculates the loss from the logits and the labels.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int64 - [batch_size].\n",
    "    Returns:\n",
    "    loss: Loss tensor of type float.\n",
    "    \"\"\"\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def training(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    Sets up the training Ops.\n",
    "    Creates a summarizer to track the loss over time in TensorBoard.\n",
    "    Creates an optimizer and applies the gradients to all trainable variables.\n",
    "    The Op returned by this function is what must be passed to the\n",
    "    `sess.run()` call to cause the model to train.\n",
    "    Args:\n",
    "    loss: Loss tensor, from loss().\n",
    "    learning_rate: The learning rate to use for gradient descent.\n",
    "    Returns:\n",
    "    train_op: The Op for training.\n",
    "    \"\"\"\n",
    "    # Add a scalar summary for the snapshot loss. (Tensor board)\n",
    "    tf.scalar_summary('loss', loss)\n",
    "\n",
    "    # Create a variable to track the global step.\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    # decay on our learning rate as we progress with training\n",
    "#     TODO: Use step for learn rate\n",
    "    learn_rate = tf.train.exponential_decay(learning_rate, global_step, 1000, 0.8)\n",
    "    tf.scalar_summary('learn rate', learning_rate)\n",
    "    \n",
    "    # Create the gradient Adam optimizer with the given learning rate.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    # Use the optimizer to apply the gradients that minimize the loss\n",
    "    # (and also increment the global step counter) as a single training step.\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "    Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor, int32 - [batch_size], with values in the\n",
    "      range [0, NUM_CLASSES).\n",
    "    Returns:\n",
    "    A scalar int32 tensor with the number of examples (out of batch_size)\n",
    "    that were predicted correctly.\n",
    "    \"\"\"\n",
    "    # For a classifier model, we can use the in_top_k Op.\n",
    "    # It returns a bool tensor with shape [batch_size] that is true for\n",
    "    # the examples where the label is in the top k (here k=1)\n",
    "    # of all logits for that example.\n",
    "    correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "    # Return the number of true entries.\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "        images, labels = inputs(batch_size, num_epochs)\n",
    "        \n",
    "        logits = inference(images, keep_prob)\n",
    "                \n",
    "        loss_op = loss(logits, labels)\n",
    "        \n",
    "        train_op = training(loss_op, learning_rate)\n",
    "\n",
    "        init_op = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "        sess = tf.Session()\n",
    "        sess.as_default()\n",
    "        \n",
    "        sess.run(init_op)\n",
    "\n",
    "        # Start populating the filename queue.\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        \n",
    "        for i in range(10):\n",
    "            # Retrieve a single instance:\n",
    "            _, ls = sess.run([train_op, loss_op], feed_dict={keep_prob : 0.5})\n",
    "\n",
    "\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "        \n",
    "        \n",
    "run_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
