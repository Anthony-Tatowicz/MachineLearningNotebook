{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.]\n",
      "0  cost 0.223913\n",
      "0  original [-0.04443906  0.76591285  0.04021515  0.03098128  0.02575706]\n",
      "0  decoded [[-0.0378052   0.27588308 -0.01657755 -0.04659388 -0.00987682]]\n",
      "100  cost 0.145057\n",
      "100  original [ 0.98557646 -0.10339412 -0.05162678  0.10254067 -0.0133595 ]\n",
      "100  decoded [[ 0.94964576 -0.41263682 -0.05048909  0.1915506   0.00563901]]\n",
      "200  cost 0.0644357\n",
      "200  original [ 0.9707531   0.05356762 -0.08395048 -0.02319878 -0.05299672]\n",
      "200  decoded [[ 0.94431245 -0.07233968 -0.13504954 -0.0097941  -0.09064266]]\n",
      "300  cost 0.0316284\n",
      "300  original [ 1.00115968  0.04527719  0.03646633 -0.0797614  -0.00564147]\n",
      "300  decoded [[ 0.93948752  0.07240146  0.03357641 -0.05875142 -0.00205949]]\n",
      "400  cost 0.0607537\n",
      "400  original [ 0.96519037  0.04196853  0.06141206  0.02577563  0.13842112]\n",
      "400  decoded [[ 0.94955581  0.17006202  0.07282116  0.03756169  0.17758247]]\n",
      "500  cost 0.0483281\n",
      "500  original [ 0.00406798  0.875572    0.00894608 -0.00406728  0.03469658]\n",
      "500  decoded [[-0.01013071  0.96979326  0.05850252 -0.00258967  0.04656007]]\n",
      "600  cost 0.0665232\n",
      "600  original [ 1.11113953 -0.1515203   0.10078424  0.06941729  0.11681403]\n",
      "600  decoded [[ 0.96721375 -0.1873123   0.08997577  0.06590695  0.11545908]]\n",
      "700  cost 0.0556105\n",
      "700  original [ 1.06713461  0.01196576  0.09211055  0.10361232  0.04667334]\n",
      "700  decoded [[ 0.9669978   0.07388894  0.10402111  0.13650551  0.06608865]]\n",
      "800  cost 0.0936427\n",
      "800  original [ 0.13078302  1.17062534 -0.10056059 -0.13084077 -0.25544913]\n",
      "800  decoded [[ 0.12749492  0.9695043  -0.0945509  -0.1343791  -0.19769394]]\n",
      "900  cost 0.0464125\n",
      "900  original [ 0.14265519  0.88167564 -0.02453412 -0.14671272 -0.06554796]\n",
      "900  decoded [[ 0.16409275  0.95903099  0.00468815 -0.09162221 -0.0446133 ]]\n",
      "1000  cost 0.109443\n",
      "1000  original [ 0.72172072 -0.14869395 -0.12787833  0.02799628 -0.107301  ]\n",
      "1000  decoded [[ 0.96611917 -0.13717249 -0.13186803  0.02801238 -0.11040863]]\n",
      "1100  cost 0.0806644\n",
      "1100  original [-0.02890439  1.04913455 -0.0084809   0.06275104  0.06853946]\n",
      "1100  decoded [[ -2.31504440e-04   9.62966502e-01  -1.53422877e-01   8.78950208e-02\n",
      "    1.19981565e-01]]\n",
      "1200  cost 0.101668\n",
      "1200  original [ 1.03091553  0.19932885 -0.0444059  -0.12195269  0.04612019]\n",
      "1200  decoded [[ 0.96388221  0.37782076 -0.14789471 -0.18447116  0.01947209]]\n",
      "1300  cost 0.0455288\n",
      "1300  original [ 0.91957597  0.0422376  -0.10712605 -0.05680544  0.0220617 ]\n",
      "1300  decoded [[ 0.96816742  0.11369553 -0.15926969 -0.05714493  0.03540073]]\n",
      "1400  cost 0.0571565\n",
      "1400  original [ 0.16706036  1.03400899 -0.01650977 -0.23947193  0.22077722]\n",
      "1400  decoded [[ 0.17759424  0.95774668 -0.01434656 -0.1845035   0.13486326]]\n",
      "1500  cost 0.0733145\n",
      "1500  original [-0.00366188  0.80508441  0.04117154 -0.19903765  0.0569798 ]\n",
      "1500  decoded [[ 0.03794242  0.96017361  0.05007325 -0.22856395  0.04512834]]\n",
      "1600  cost 0.0575058\n",
      "1600  original [-0.18196431  1.07459989  0.1281498  -0.11351854  0.03616351]\n",
      "1600  decoded [[-0.25038061  0.96741211  0.13191754 -0.12573922  0.05034241]]\n",
      "1700  cost 0.0631264\n",
      "1700  original [ 1.09879621 -0.24691168  0.05201259 -0.16368768  0.00747203]\n",
      "1700  decoded [[ 0.97131073 -0.30734503  0.05376917 -0.16778938  0.00768744]]\n",
      "1800  cost 0.0527759\n",
      "1800  original [ 0.9959804  -0.06337626  0.08731217  0.0267058   0.07597471]\n",
      "1800  decoded [[ 0.97051525 -0.04456344  0.17551497  0.08116779  0.12264629]]\n",
      "1900  cost 0.0644828\n",
      "1900  original [-0.06857338  0.960457    0.01850439 -0.11492025  0.05178744]\n",
      "1900  decoded [[ 0.00892663  0.96701121  0.02850123 -0.23342626  0.07622816]]\n",
      "2000  cost 0.0829174\n",
      "2000  original [ 1.02703049 -0.06116928  0.06974406 -0.01728705 -0.09200458]\n",
      "2000  decoded [[ 0.97567499  0.00395201  0.0773688   0.12831119 -0.01300274]]\n",
      "2100  cost 0.0879238\n",
      "2100  original [ 1.10112471  0.30642832 -0.03154184  0.11119026 -0.10935858]\n",
      "2100  decoded [[ 0.96218592  0.42513132  0.02368291  0.15022266 -0.13553658]]\n",
      "2200  cost 0.0774375\n",
      "2200  original [ 0.09840256  1.09560738 -0.02077903 -0.02007438  0.06676267]\n",
      "2200  decoded [[ 0.1952742   0.9653281   0.03421674  0.00350603  0.07350282]]\n",
      "2300  cost 0.0601725\n",
      "2300  original [-0.11817182  0.85732336 -0.04213301  0.00723927  0.03514328]\n",
      "2300  decoded [[-0.12615064  0.97167772  0.02200679  0.00630574  0.06426879]]\n",
      "2400  cost 0.0714412\n",
      "2400  original [ 0.83315861  0.03615621  0.15508433  0.05179707 -0.02690062]\n",
      "2400  decoded [[ 0.94997466  0.144325    0.14479229  0.04501662 -0.0314689 ]]\n",
      "2500  cost 0.0119429\n",
      "2500  original [-0.12873298  0.97069586  0.0449031   0.05789859  0.21109409]\n",
      "2500  decoded [[-0.12103246  0.96903515  0.05256469  0.05769841  0.18675552]]\n",
      "2600  cost 0.0643512\n",
      "2600  original [-0.00555549  1.07048969  0.16229142 -0.03515685  0.06693461]\n",
      "2600  decoded [[ 0.06828477  0.95758146  0.20704964 -0.04254619  0.08807355]]\n",
      "2700  cost 0.0948761\n",
      "2700  original [ 1.16122736  0.09492813 -0.10039673 -0.09150583 -0.01485175]\n",
      "2700  decoded [[ 0.9615823   0.15922697 -0.13045268 -0.10144164 -0.01842861]]\n",
      "2800  cost 0.0816725\n",
      "2800  original [ 1.1397438  -0.23469696 -0.1529444   0.03711137 -0.04809477]\n",
      "2800  decoded [[ 0.97497731 -0.31303388 -0.15944086  0.03269284 -0.05046433]]\n",
      "2900  cost 0.0815602\n",
      "2900  original [ 1.11500052  0.03555895  0.15730775 -0.05235332  0.03263157]\n",
      "2900  decoded [[ 0.96848768  0.05809553  0.23263805 -0.11106072  0.07916357]]\n",
      "3000  cost 0.07926\n",
      "3000  original [-0.0785101   1.03142845  0.26485154 -0.05330758  0.2972387 ]\n",
      "3000  decoded [[-0.05778199  0.96674067  0.12302253 -0.04543164  0.21588108]]\n",
      "3100  cost 0.0499427\n",
      "3100  original [ 1.01995278 -0.04360468  0.11878304 -0.02186964  0.17249662]\n",
      "3100  decoded [[ 0.97124261 -0.02266782  0.07785839 -0.03721466  0.08446237]]\n",
      "3200  cost 0.0682283\n",
      "3200  original [ 1.09569987 -0.00772891  0.26036771  0.07304323 -0.15334687]\n",
      "3200  decoded [[ 0.96747142 -0.07910646  0.29680133  0.05834537 -0.16730358]]\n",
      "3300  cost 0.0744628\n",
      "3300  original [ 1.11057552 -0.11264309  0.0184491   0.10309966  0.0233104 ]\n",
      "3300  decoded [[ 0.97218585 -0.20507212  0.01410281  0.10608817  0.02235534]]\n",
      "3400  cost 0.0741221\n",
      "3400  original [ 0.11116561  1.02771922 -0.05339266 -0.06068094 -0.0369493 ]\n",
      "3400  decoded [[ 0.22754961  0.96646678 -0.01590032 -0.1538775  -0.0460091 ]]\n",
      "3500  cost 0.0860242\n",
      "3500  original [-0.1735767   1.12730693 -0.09706845  0.08429822 -0.11454607]\n",
      "3500  decoded [[-0.2730349   0.96779537 -0.12524785  0.10492059 -0.13565485]]\n",
      "3600  cost 0.0473698\n",
      "3600  original [ 0.04282471  0.95784003 -0.08191978 -0.24125365 -0.02364113]\n",
      "3600  decoded [[ 0.05672433  0.9712531  -0.07381629 -0.33955482  0.00978914]]\n",
      "3700  cost 0.07144\n",
      "3700  original [ 0.10726184  0.95347603  0.02992504  0.10005049  0.0511935 ]\n",
      "3700  decoded [[ 0.22712353  0.95698255  0.03518637  0.17285773  0.12742127]]\n",
      "3800  cost 0.0797157\n",
      "3800  original [ 1.06869634 -0.13835126  0.11720372 -0.03018401  0.1764935 ]\n",
      "3800  decoded [[ 0.95903409 -0.21129261  0.12302554 -0.02548287  0.29637134]]\n",
      "3900  cost 0.0809489\n",
      "3900  original [-0.13357175  1.015644    0.1094829  -0.09473574  0.06080325]\n",
      "3900  decoded [[-0.24422674  0.97147548  0.18408014 -0.20836046  0.05116586]]\n",
      "4000  cost 0.0345867\n",
      "4000  original [-0.12338851  1.02739267  0.022426    0.10196891 -0.07073257]\n",
      "4000  decoded [[-0.1253604   0.97585315  0.03728749  0.15350728 -0.04966332]]\n",
      "4100  cost 0.0646581\n",
      "4100  original [-0.07452219  1.03170347  0.06461354 -0.08047191 -0.13031645]\n",
      "4100  decoded [[-0.11774042  0.9705826   0.06433201 -0.13987704 -0.23880956]]\n",
      "4200  cost 0.0882116\n",
      "4200  original [ 0.33398897  1.07624136 -0.02806802 -0.05817899 -0.01657301]\n",
      "4200  decoded [[ 0.49351457  0.96295637 -0.04019479 -0.03717884 -0.02260758]]\n",
      "4300  cost 0.0449147\n",
      "4300  original [ 0.06540569  1.0420444   0.07524937 -0.01002733 -0.09361024]\n",
      "4300  decoded [[-0.01180791  0.97966516  0.06176635 -0.01607957 -0.08972388]]\n",
      "4400  cost 0.0782282\n",
      "4400  original [-0.08333507  0.96542313 -0.10688559 -0.10394448 -0.07040476]\n",
      "4400  decoded [[-0.00897562  0.97731245 -0.08598695  0.01044729 -0.17720054]]\n",
      "4500  cost 0.0463062\n",
      "4500  original [ 0.05468644  1.06854557 -0.06000883  0.08430643  0.04372494]\n",
      "4500  decoded [[ 0.0975884   0.97767246 -0.04641899  0.08098859  0.02305814]]\n",
      "4600  cost 0.0335146\n",
      "4600  original [ 0.92067536 -0.09567182  0.15425451 -0.07160374  0.08331176]\n",
      "4600  decoded [[ 0.96847504 -0.11362129  0.11353238 -0.05473393  0.05065789]]\n",
      "4700  cost 0.0975173\n",
      "4700  original [ 1.16797318  0.12554309  0.09397595  0.00553962  0.02759946]\n",
      "4700  decoded [[ 0.96324551  0.18839654  0.12336311 -0.02040772  0.01546165]]\n",
      "4800  cost 0.0621725\n",
      "4800  original [ 0.07024836  1.04396717 -0.01352429 -0.10841336 -0.09953041]\n",
      "4800  decoded [[ 0.07145835  0.97432679  0.01204435 -0.19702139 -0.17680079]]\n",
      "4900  cost 0.0401206\n",
      "4900  original [  8.35355425e-02   8.82910342e-01   5.04332400e-02   1.40931889e-01\n",
      "   8.39082322e-04]\n",
      "4900  decoded [[  8.45180824e-02   9.72531736e-01   4.70158421e-02   1.39259115e-01\n",
      "   -9.85767692e-05]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Deep Auto-Encoder implementation\n",
    "\t\n",
    "\tAn auto-encoder works as follows:\n",
    "\tData of dimension k is reduced to a lower dimension j using a matrix multiplication:\n",
    "\tsoftmax(W*x + b)  = x'\n",
    "\t\n",
    "\twhere W is matrix from R^k --> R^j\n",
    "\tA reconstruction matrix W' maps back from R^j --> R^k\n",
    "\tso our reconstruction function is softmax'(W' * x' + b') \n",
    "\tNow the point of the auto-encoder is to create a reduction matrix (values for W, b) \n",
    "\tthat is \"good\" at reconstructing  the original data. \n",
    "\tThus we want to minimize  ||softmax'(W' * (softmax(W *x+ b)) + b')  - x||\n",
    "\tA deep auto-encoder is nothing more than stacking successive layers of these reductions.\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "def create(x, layer_sizes):\n",
    "\n",
    "\t# Build the encoding layers\n",
    "\tnext_layer_input = x\n",
    "\n",
    "\tencoding_matrices = []\n",
    "\tfor dim in layer_sizes:\n",
    "\t\tinput_dim = int(next_layer_input.get_shape()[1])\n",
    "\n",
    "\t\t# Initialize W using random values in interval [-1/sqrt(n) , 1/sqrt(n)]\n",
    "\t\tW = tf.Variable(tf.random_uniform([input_dim, dim], -1.0 / math.sqrt(input_dim), 1.0 / math.sqrt(input_dim)))\n",
    "\n",
    "\t\t# Initialize b to zero\n",
    "\t\tb = tf.Variable(tf.zeros([dim]))\n",
    "\n",
    "\t\t# We are going to use tied-weights so store the W matrix for later reference.\n",
    "\t\tencoding_matrices.append(W)\n",
    "\n",
    "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
    "\n",
    "\t\t# the input into the next layer is the output of this layer\n",
    "\t\tnext_layer_input = output\n",
    "\n",
    "\t# The fully encoded x value is now stored in the next_layer_input\n",
    "\tencoded_x = next_layer_input\n",
    "\n",
    "\t# build the reconstruction layers by reversing the reductions\n",
    "\tlayer_sizes.reverse()\n",
    "\tencoding_matrices.reverse()\n",
    "\n",
    "\n",
    "\tfor i, dim in enumerate(layer_sizes[1:] + [ int(x.get_shape()[1])]) :\n",
    "\t\t# we are using tied weights, so just lookup the encoding matrix for this step and transpose it\n",
    "\t\tW = tf.transpose(encoding_matrices[i])\n",
    "\t\tb = tf.Variable(tf.zeros([dim]))\n",
    "\t\toutput = tf.nn.tanh(tf.matmul(next_layer_input,W) + b)\n",
    "\t\tnext_layer_input = output\n",
    "\n",
    "\t# the fully encoded and reconstructed value of x is here:\n",
    "\treconstructed_x = next_layer_input\n",
    "\n",
    "\treturn {\n",
    "\t\t'encoded': encoded_x,\n",
    "\t\t'decoded': reconstructed_x,\n",
    "\t\t'cost' : tf.sqrt(tf.reduce_mean(tf.square(x-reconstructed_x)))\n",
    "\t}\n",
    "\n",
    "def simple_test():\n",
    "\tsess = tf.Session()\n",
    "\tx = tf.placeholder(\"float\", [None, 4])\n",
    "\tautoencoder = create(x, [2])\n",
    "\tinit = tf.initialize_all_variables()\n",
    "\tsess.run(init)\n",
    "\ttrain_step = tf.train.GradientDescentOptimizer(0.01).minimize(autoencoder['cost'])\n",
    "\n",
    "\n",
    "\t# Our dataset consists of two centers with gaussian noise w/ sigma = 0.1\n",
    "\tc1 = np.array([0,0,0.5,0])\n",
    "\tc2 = np.array([0.5,0,0,0])\n",
    "\n",
    "\t# do 1000 training steps\n",
    "\tfor i in range(2000):\n",
    "\t\t# make a batch of 100:\n",
    "\t\tbatch = []\n",
    "\t\tfor j in range(100):\n",
    "\t\t\t# pick a random centroid\n",
    "\t\t\tif (random.random() > 0.5):\n",
    "\t\t\t\tvec = c1\n",
    "\t\t\telse:\n",
    "\t\t\t\tvec = c2\n",
    "  \t\t\tbatch.append(np.random.normal(vec, 0.1))\n",
    "\t\tsess.run(train_step, feed_dict={x: np.array(batch)})\n",
    "\t\tif i % 100 == 0:\n",
    "\t\t\tprint i, \" cost\", sess.run(autoencoder['cost'], feed_dict={x: batch})\n",
    "\n",
    "\n",
    "def deep_test():\n",
    "\tsess = tf.Session()\n",
    "\tstart_dim = 5\n",
    "\tx = tf.placeholder(\"float\", [None, start_dim])\n",
    "\tautoencoder = create(x, [4, 3, 2])\n",
    "\tinit = tf.initialize_all_variables()\n",
    "\tsess.run(init)\n",
    "\ttrain_step = tf.train.GradientDescentOptimizer(0.5).minimize(autoencoder['cost'])\n",
    "\n",
    "\n",
    "\t# Our dataset consists of two centers with gaussian noise w/ sigma = 0.1\n",
    "\tc1 = np.zeros(start_dim)\n",
    "\tc1[0] = 1\n",
    "\n",
    "\tprint c1\n",
    "\n",
    "\tc2 = np.zeros(start_dim)\n",
    "\tc2[1] = 1\n",
    "\n",
    "\t# do 1000 training steps\n",
    "\tfor i in range(5000):\n",
    "\t\t# make a batch of 100:\n",
    "\t\tbatch = []\n",
    "\t\tfor j in range(1):\n",
    "\t\t\t# pick a random centroid\n",
    "\t\t\tif (random.random() > 0.5):\n",
    "\t\t\t\tvec = c1\n",
    "\t\t\telse:\n",
    "\t\t\t\tvec = c2\n",
    "  \t\t\tbatch.append(np.random.normal(vec, 0.1))\n",
    "\t\tsess.run(train_step, feed_dict={x: np.array(batch)})\n",
    "\t\tif i % 100 == 0:\n",
    "\t\t\tprint i, \" cost\", sess.run(autoencoder['cost'], feed_dict={x: batch})\n",
    "\t\t\tprint i, \" original\", batch[0]\n",
    "\t\t\tprint i, \" decoded\", sess.run(autoencoder['decoded'], feed_dict={x: batch})\n",
    "\t\t\t\n",
    "if __name__ == '__main__':\n",
    "\tdeep_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
