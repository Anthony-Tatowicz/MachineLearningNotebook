{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bays Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bays Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple example of the Naive Bays algorithm from sci-kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 2D vectors that we are feeding in an as argument \"X\" which are then matched with a scalar as the expected output \"Y\". The GaussianNB algorithm then finds an equation that models the training data so that it can predict unknown data with the predict function/the model it just generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classifying \"real\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an example of classifying real data with Naive Bays and what the output will look like, this also has reuseable functions that we will use when we are looking at SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Imports we will need\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('agg')\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeTerrainData(n_points=1000):\n",
    "### make the toy dataset\n",
    "    random.seed(42)\n",
    "    grade = [random.random() for ii in range(0, n_points)]\n",
    "    bumpy = [random.random() for ii in range(0, n_points)]\n",
    "    error = [random.random() for ii in range(0, n_points)]\n",
    "    y = [round(grade[ii] * bumpy[ii] + 0.3 + 0.1 * error[ii]) for ii in range(0, n_points)]\n",
    "    \n",
    "    for ii in range(0, len(y)):\n",
    "        if grade[ii] > 0.8 or bumpy[ii] > 0.8:\n",
    "            y[ii] = 1.0\n",
    "\n",
    "### split into train/test sets\n",
    "    X = [[gg, ss] for gg, ss in zip(grade, bumpy)]\n",
    "    split = int(0.75 * n_points)\n",
    "    X_train = X[0:split]\n",
    "    X_test  = X[split:]\n",
    "    y_train = y[0:split]\n",
    "    y_test  = y[split:]\n",
    "\n",
    "    grade_sig = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    bumpy_sig = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==0]\n",
    "    grade_bkg = [X_train[ii][0] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "    bumpy_bkg = [X_train[ii][1] for ii in range(0, len(X_train)) if y_train[ii]==1]\n",
    "\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    test_data = {\n",
    "        \"fast\": {\n",
    "            \"grade\": grade_sig, \n",
    "            \"bumpiness\": bumpy_sig\n",
    "        }, \n",
    "        \"slow\": {\n",
    "            \"grade\": grade_bkg, \n",
    "            \"bumpiness\": bumpy_bkg\n",
    "        }\n",
    "    }\n",
    "\n",
    "    #    return training_data, test_data\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(features_train, labels_train):   \n",
    "    ### create classifier\n",
    "    ### fit the classifier on the training features and labels\n",
    "    ### return the fit classifier\n",
    "    clf = GaussianNB()\n",
    "    return clf.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def displayAccuracy(pred, label_test):\n",
    "    #Display our accuracy metric\n",
    "    acc = accuracy_score(pred, labels_test)\n",
    "    print(acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prettyPicture(clf, X_test, y_test, name):\n",
    "    x_min = 0.0; x_max = 1.0\n",
    "    y_min = 0.0; y_max = 1.0\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    h = .01  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "    #, cmap=pl.cm.seismic\n",
    "    plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "    # Plot also the test points\n",
    "    grade_sig = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    bumpy_sig = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==0]\n",
    "    grade_bkg = [X_test[ii][0] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "    bumpy_bkg = [X_test[ii][1] for ii in range(0, len(X_test)) if y_test[ii]==1]\n",
    "\n",
    "    plt.scatter(grade_sig, bumpy_sig, color = \"b\", label=\"fast\")\n",
    "    plt.scatter(grade_bkg, bumpy_bkg, color = \"r\", label=\"slow\")\n",
    "#     plt.legend()\n",
    "    plt.xlabel(\"bumpiness\")\n",
    "    plt.ylabel(\"grade\")\n",
    "\n",
    "    plt.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003 s\n",
      "0.884\n"
     ]
    }
   ],
   "source": [
    "#Here is where we generate our data we will use throughout.\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "### the training data (features_train, labels_train) have both \"fast\" and \"slow\" points mixed\n",
    "### in together--separate them so we can give them different colors in the scatterplot,\n",
    "### and visually identify them\n",
    "grade_fast = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "bumpy_fast = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==0]\n",
    "grade_slow = [features_train[ii][0] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "bumpy_slow = [features_train[ii][1] for ii in range(0, len(features_train)) if labels_train[ii]==1]\n",
    "\n",
    "t0 = time()\n",
    "clf = classify(features_train, labels_train)\n",
    "print(round(time() - t0, 3), 's')\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "displayAccuracy(pred, labels_test)\n",
    "\n",
    "### draw the decision boundary with the text points overlaid\n",
    "prettyPicture(clf, features_test, labels_test, \"NB.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Niave Bays classification fits reasonably well (88.4%), with the curved line separating the blue/red data pionts or in this case the areas one should travel fast or slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![NB](https://drive.google.com/uc?id=0B-bXZ3DDDLdWZ1ZMUW9Rby1ubW8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Simple Example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have an example straight from the docs of sklearn, this will act as my reference while I'm experimenting with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "X = [[0, 0], [1, 1]]\n",
    "y = [0, 1]\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)  \n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "\n",
    "print(clf.predict([[2., 2.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Technical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM is a powerful classification technique that can outperform Naive Bays (NB) in many instance. The trick behind SVM is to map our $n$ deminsional training data to a higher demisions $m$, so that we can find a linear relation that was not possible in the original demisionss, then we can map back to our origial $n$ deminsions in the form of a non-linear relation. The mapping is the job of the kernel function typically denoted $k(x,y)$, which can come in many different forms. The definition and kernels used are denoted below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ k: \\mathcal X \\times \\mathcal X \\rightarrow \\mathbb R $$\n",
    "\n",
    "$$ \\sum_{i,j=0}^n c_i c_j k(x_i,x_j) \\ge 0 \\quad n \\in \\mathbb N \\quad x_1,x_2, \\ldots ,x_n \\in \\mathcal X \\quad c_1,c_2, \\ldots ,c_n \\in \\mathbb R$$\n",
    "\n",
    "$$ linear: \\quad k(x,y) = x^Ty$$\n",
    "\n",
    "$$ rbf: \\quad k(x,y) = e^{\\frac{||x-y||^2}{-2\\sigma^2}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM does come at a cost, it's efficency is $O(n^3)$ on the training step, which can take much longer than NB in certain cases, also we must note that SVM depends on $k(x,y)$, like the $linear, rbf, polynomial, sigmoid$ kernels, these kernels have different efficencies and performance, depeding on the data. For example $linear$ will be much faster than $rbf$, since $rbf$ can be non-linear and thus more complex to calculate, however if the data has a more $linear$ classification a $linear$ kernel can out perform an $rbf$ kernel, because of overfitting. The same idea applies to the other kernels as well, so one must understand the data to choose the best option in terms of efficency and performace. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM using our data from earlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of linear SVM using the generated data, with the gamma parameter set to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "\n",
    "features_train, labels_train, features_test, labels_test = makeTerrainData()\n",
    "\n",
    "\n",
    "########################## SVM #################################\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear', gamma=1.0)\n",
    "\n",
    "\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "displayAccuracy(pred, labels_test)\n",
    "\n",
    "#Generate a picture of the data\n",
    "prettyPicture(clf, features_test, labels_test, \"SVM.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM](https://drive.google.com/uc?id=0B-bXZ3DDDLdWOGVMV0REN29Sc2M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets try the RBF (a non linear svm kernel) and lets play with the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM kernel = rbf, C = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', C=1.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "displayAccuracy(pred, labels_test)\n",
    "\n",
    "prettyPicture(clf, features_test, labels_test, \"rbf_SM_C.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM_SM](https://drive.google.com/uc?id=0B-bXZ3DDDLdWUVFKTW1OWjRQV3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM kernel = rbf, C = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.924\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', C=1000.0)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "displayAccuracy(pred, labels_test)\n",
    "\n",
    "prettyPicture(clf, features_test, labels_test, \"rbf_L_C.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM_L](https://drive.google.com/uc?id=0B-bXZ3DDDLdWdENUU09ZcjR5OEk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM kernel = rb, C = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.137 s\n",
      "0.944\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(kernel='rbf', C=100000.0)\n",
    "t0 = time()\n",
    "clf.fit(features_train, labels_train)\n",
    "print(round(time() - t0, 3), 's')\n",
    "\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "displayAccuracy(pred, labels_test)\n",
    "\n",
    "prettyPicture(clf, features_test, labels_test, \"rbf_XL_C.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM_L](https://drive.google.com/uc?id=0B-bXZ3DDDLdWbE1iWFZZbVBXQWM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conculsion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that as the $C \\rightarrow \\infty$ the classification performance improves and becomes more complex. It is shown that SVM can outperform NB, but also runs the risk of overfitting and poor efficency, however in this example SVM was better than NB. SVM allowed for better performance, at a relativly low cost in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/svm.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "https://www.udacity.com/wiki/ud120"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
