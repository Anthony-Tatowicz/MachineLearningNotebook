{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified ../data/text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, '../data/' + filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293951 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "xh iempepppoig lome tbuitqi acuoumanb meags nxhu fldfiad j o opawjrzqusorfbtoyva\n",
      "o  smfe xfjjbgw akarobist  g r oxzidedrnosjgucdl vu z ih ew g   rknx vgabvqeah d\n",
      "pswvcirrtprgidd appe ogx iu tsfarproifph il hiiy xtegribobdmoeei bmu ommt oindfm\n",
      "ctwpacwwecuv  tonxutvzqe cdnxijeo coqv  isa kk gevtzxvnufanm    khyhwwee  ysaec \n",
      "grco  yoygd ah hnoexoggghiiouhgp nil ljkeieaun awofaryff v ifukvujhqpseae a oext\n",
      "================================================================================\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 1000: 2.013022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 2000: 1.723194 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 3000: 1.661788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4000: 1.651622 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5000: 1.624209 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6000: 1.578144 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 7000: 1.570132 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 8000: 1.579812 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 9000: 1.570998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 10000: 1.596596 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "x the alis have sick the sercals where ifers over edware cynservey mancent is ph\n",
      "nolan the was of the revercher of grours of uper schem it as to dislovists of ex\n",
      "rates altramah in the directmentation of the test folds wrass woble he attene in\n",
      "jom look sand would abbu varies and torb by prosect borthwut of the tritions the\n",
      "pich the reduceing an encorcon agenttion and csysrasss it was boyang ausor u str\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 11000: 1.594836 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 12000: 1.586219 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 13000: 1.578607 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 14000: 1.577135 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 15000: 1.580811 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 16000: 1.580800 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 17000: 1.593026 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 18000: 1.577798 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 19000: 1.585866 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 20000: 1.596809 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "cre in canta and senents indessited with the gaming sureagen atthologically thra\n",
      "zast from an actly wate operates the most wilding is beingle two five rassed bat\n",
      "ddpt sarrity freared to bacs was body time surbernency preserie in ecrititizity \n",
      "persal bis databte is a wavin britalism opedant chabmeses slagic wasy conreduin \n",
      "liters heat to a fe breakbes quy but mured the settic kay when the provicfic ana\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 21000: 1.602185 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 22000: 1.584474 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 23000: 1.581168 learning rate: 0.001000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 24000: 1.597844 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 25000: 1.582142 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 26000: 1.582135 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 27000: 1.576255 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 28000: 1.600325 learning rate: 0.000100\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 29000: 1.586553 learning rate: 0.000100\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 30000: 1.587760 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.20\n",
      "================================================================================\n",
      "a argest beginns philosons and several imenary success traks lucl sevolf two zer\n",
      "usuary regardes backl to real contraquourdementa nigrav influence is manisticle \n",
      "not as damce the galls outlon one two seven zero detyback locple quagurts loctiv\n",
      "hafsplarthing space to and pooters disolbbote whele toperate bach write choon of\n",
      "varts field developed a differents by not is supers the memory deferencipa guide\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 31000: 1.574062 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 32000: 1.582280 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 33000: 1.581301 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 34000: 1.581777 learning rate: 0.000010\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 35000: 1.591320 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 36000: 1.590672 learning rate: 0.000001\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 37000: 1.593804 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 38000: 1.606271 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 39000: 1.597433 learning rate: 0.000001\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 40000: 1.557116 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.36\n",
      "================================================================================\n",
      "way onlywathon he eight three internstonel indumepented islamyd appoot of pposed\n",
      "gree a prevists this tellhis ox way aguectucled to authatic is less of the great\n",
      "was electly meth ho reff impitoods with eathes them the nest georating jechot lo\n",
      "timis concerts stour cease visions leading in them the case motreass diltic augu\n",
      "ation of subseam puenter the if apresia of more there who after hunarch tracus q\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 41000: 1.567066 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 42000: 1.577009 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 43000: 1.591453 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 44000: 1.555706 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 45000: 1.558695 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 46000: 1.601562 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 47000: 1.606317 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 48000: 1.601361 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 49000: 1.583420 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 50000: 1.590654 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "cialists is the two swent they divolamps eight three on the luve any ffockmenthi\n",
      "y a simple the receivor bojopening nate side sound constatutersublaving teges fi\n",
      "y alsup overfucified amil german on dnogter spear object hamoughnoam wave secure\n",
      "vilus in the abmation the christian partial interrestr of abi one nine nine zero\n",
      "cals will out bapanted are is the repecty patiators by phetic subjects and neurr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 51000: 1.600315 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 52000: 1.609819 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 53000: 1.582547 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 54000: 1.573831 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 55000: 1.586701 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 56000: 1.592024 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 57000: 1.590096 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 58000: 1.603457 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 59000: 1.596820 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 60000: 1.600557 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.31\n",
      "================================================================================\n",
      "jan with a been made follow battlenceping of hir includes lewell livanure of inr\n",
      "anding of transine a see five five austriess hylock more and depointaria singed \n",
      "best valmes of the circhlias sifferd somether six six one two sterniqult compoli\n",
      "hasachief during cherfiwarist communic quide futto one eight seven nightlers be \n",
      "ch jelor daj empire flagob there four the informations of been galfon its pre no\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 61000: 1.592099 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 62000: 1.597231 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 63000: 1.591227 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 64000: 1.585996 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 65000: 1.573321 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 66000: 1.575271 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 67000: 1.591523 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 68000: 1.613072 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 69000: 1.604531 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 70000: 1.590007 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.07\n",
      "================================================================================\n",
      "y codting such as oute cophelogys been of more powed over milds methemps dulu eg\n",
      "ques and the roce and a marala isly earbann of armenout undises sent that streap\n",
      "zaundael san notal of orthology terplogy we famague angelly poloneraliared of en\n",
      "gny as new the marnia realled but word the hamid fool sizes order suplese relati\n",
      "l not the sukfines officially one nine two shasilus sumong in miks albulity unde\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 71000: 1.592313 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 72000: 1.598010 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 73000: 1.593770 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 74000: 1.583538 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 75000: 1.588167 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 76000: 1.592096 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 77000: 1.598179 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 78000: 1.587741 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 79000: 1.584462 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 80000: 1.587517 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "usonulour longovers s its ricational boind politicolation of the tug additationa\n",
      "ware and woidhlum attement towerul of pathy used than to he irtasta speniob to o\n",
      "b police most yetwo the victo zerozl tullided and interniteping and frictimicang\n",
      "fortu of the site works witz planstra fac set of the courses used from thein bec\n",
      "s subject of fortex the accorporated in the dubell perman in the skw recome play\n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 81000: 1.584366 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 82000: 1.595744 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 83000: 1.593660 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 84000: 1.593611 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 85000: 1.603898 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 86000: 1.582833 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 87000: 1.579821 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 88000: 1.593978 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 89000: 1.580137 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 90000: 1.590412 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      " detacks dati george freedher for in and m water his one nine eight five five se\n",
      "lu rost and l stune sembers than standing that specik with aght genents is suctu\n",
      "wity of the staint who of his dics pulyally andarity king contrilities reference\n",
      "vateding by postea ithese sonth a three forcing way fuels weightaff nanies in wi\n",
      "ing accostories europe of irth onel one of the probuters of sutiod turtlase and \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 91000: 1.575000 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 92000: 1.575233 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 93000: 1.603729 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 94000: 1.626284 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 95000: 1.615476 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 96000: 1.605870 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 97000: 1.597655 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 98000: 1.590490 learning rate: 0.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 99000: 1.589786 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 100000: 1.570458 learning rate: 0.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "x one eight one six zero sisteng of sected an on same is molos x rible f scrooda\n",
      "mill latway statismy and a east ssuke zon of french cupi stories stonition south\n",
      "janzend my their mid of the other aatural orders of was asharder a auther richle\n",
      "use in appointing in wordiginate mode is this glist by out two bovarn was been t\n",
      "y as two four per at the chance the ostics takexe with econed projagnalism benn \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
